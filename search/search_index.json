{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MURPHY - Multiresolution Multiphysics Solver Image caption MURPHY is a library to perform high-performance computing simulations of partial differential equations on multiresolution adaptive grids. It uses an immersed interface method to handle domain boundaries. MURPHY is written in C++ and MPI . We have provided more information about the motivation and theoretical background on a couple of key components: multiresolution grid adaptation through wavelets high-order discretization of boundaries through immersed interface method high performance computing using one-sided MPI communication integration with FLUPS for Fast Fourier Transforms (uniform grids only) Further, we have a couple of clients: vortexdyn : discretization of 3D Navier-Stokes equations using vorticity-velocity formulation TBD The scientific paper can be found here while the Github code is here .","title":"Home"},{"location":"#murphy-multiresolution-multiphysics-solver","text":"Image caption MURPHY is a library to perform high-performance computing simulations of partial differential equations on multiresolution adaptive grids. It uses an immersed interface method to handle domain boundaries. MURPHY is written in C++ and MPI . We have provided more information about the motivation and theoretical background on a couple of key components: multiresolution grid adaptation through wavelets high-order discretization of boundaries through immersed interface method high performance computing using one-sided MPI communication integration with FLUPS for Fast Fourier Transforms (uniform grids only) Further, we have a couple of clients: vortexdyn : discretization of 3D Navier-Stokes equations using vorticity-velocity formulation TBD The scientific paper can be found here while the Github code is here .","title":"MURPHY - Multiresolution Multiphysics Solver"},{"location":"compilationflags/","text":"Compilation Flags Some compilations flags are available to change the behavior of the code: -DBLOCK_GS=X sets the number of ghost points to use (default X=2 ) -DWAVELET_N=X dictates the interpolation order of the wavelets (default X=2 ) -DWAVELET_NT=X dictates the moment order of the wavelets (default X=2 ) -DLOG_ALLRANKS will enable log on every processor. By default, only the master logs -DVERBOSE enable extended logs -DNDEBUG disable the assertion checks and the other debuging sections -DLOG_MUTE disable every logs -DCOLOR_PROF enable color output in the profiler (disabled by default) -DMPI_NONASYNC ask for non-asynchronous MPI calls + remove the fancy optimization options -DOLD_GCC replaces the aligned_alloc by posix_memalign to support GCC 8 To use them, you can append the make command, e.g. to change the wavelet behavior make OPTS = \"-DBLOCK_GS=4 -DWAVELET_N=4\" or add it to the ARCH file: override OPTS += -DBLOCK_GS = 4 -DWAVELET_N = 2 In doubts, you should always run make info to test the different options. the order of the wavelet must be compatible with the number of ghost points!","title":"Compilation Flags"},{"location":"compilationflags/#compilation-flags","text":"Some compilations flags are available to change the behavior of the code: -DBLOCK_GS=X sets the number of ghost points to use (default X=2 ) -DWAVELET_N=X dictates the interpolation order of the wavelets (default X=2 ) -DWAVELET_NT=X dictates the moment order of the wavelets (default X=2 ) -DLOG_ALLRANKS will enable log on every processor. By default, only the master logs -DVERBOSE enable extended logs -DNDEBUG disable the assertion checks and the other debuging sections -DLOG_MUTE disable every logs -DCOLOR_PROF enable color output in the profiler (disabled by default) -DMPI_NONASYNC ask for non-asynchronous MPI calls + remove the fancy optimization options -DOLD_GCC replaces the aligned_alloc by posix_memalign to support GCC 8 To use them, you can append the make command, e.g. to change the wavelet behavior make OPTS = \"-DBLOCK_GS=4 -DWAVELET_N=4\" or add it to the ARCH file: override OPTS += -DBLOCK_GS = 4 -DWAVELET_N = 2 In doubts, you should always run make info to test the different options. the order of the wavelet must be compatible with the number of ghost points!","title":"Compilation Flags"},{"location":"compiling/","text":"Compiling the code There are two ways to compile and run the code. The first one is to use the docker container, which pulls in all dependencies in a controlled environment. This is a good way to get started and for local development. The second one is to compile the dependencies and code outside of the docker container, which is the preferred way for large-scale simulations. Docker + VSCode Within Visual Studio Code, you can use the remote container extension to open, build and run the code directly into a Docker container. To do so, install the Remote Container extension in VSCode, and open the folder where you cloned the murphy code in VSCode. A message will pop up that you can accept, which will download and build the container, and open a terminal in that container. You can go in that terminal, navigate to the main murphy folder, and type ` MAKE_ARCH = make_arch/make.docker_gcc make -j and compilation should start and finish without errors. Direct compilation To compile the code natively on any machine, you have to first download and install the required dependencies . Once you have those, you can create your own architecture file for make to call, which should be placed in the make_arch/ folder. You can take a look at some of the examples that are there. Specifically, you can specify the following variables: flags CXX_FLAGS : the compilation flags you want to use (default: none ) GIT_COMMIT : the commit associated to the current version of the code (default: taken from the current repo using git describe --always --dirty ) P4est library P4EST_INC : the folder with the p4est headers (default /usr/include ) P4EST_LIB : the folder with the p4est libraries (default /usr/lib ) P4EST_LIBNAME : the name of the p4est library on your system (default: -lsc -lp4est ) HDF5 library HDF5_INC : the folder with the hdf5 headers (default /usr/include ) HDF5_LIB : the folder with the hdf5 libraries (default /usr/lib ) HDF5_LIBNAME : the name of the hdf5 library on your system (default: -lhdf5 ) FFTW library FFTW_INC : the folder with the fftw headers (default /usr/include ) FFTW_LIB : the folder with the fftw libraries (default /usr/lib ) FFTW_LIBNAME : the name of the fftw library on your system (default: -lfftw3_omp -lfftw3 ) Google Test library GTEST_INC : the folder with the google test headers (default /usr/include ) GTEST_LIB : the folder with the google test libraries (default /usr/lib ) GTEST_LIBNAME : the name of the google test library on your system (default: -lgtest ) Testing See the details about automatic testing in this page . To run the tests, you need to recompile all the sources (it's important to delete the old ones if you want to retrieve) make destroy ARCH_FILE=make_arch/make.docker_gcc_test make test -j Then you can lunch the tests ./murphy_test Documentation To get the doxygen documentation, go to the main folder and enter doxygen doc/Doxyfile You will need dot to get a visual graphs (see HAVE_DOT option in the Doxyfile). On MacOS, you can install it using homebrew: brew install graphviz . Docker troubleshoot the compilation fails and the error indicates that files change sizes during the compilation: you might be the victim of slow disk access between docker and your laptop. To solve that, you need to follow this and set the consistency to delegated . Building on clusters here are the links to the details needed to build murphy on different clusters - Engaging - NIC 5 - Lemaitre 3","title":"Compilation"},{"location":"compiling/#compiling-the-code","text":"There are two ways to compile and run the code. The first one is to use the docker container, which pulls in all dependencies in a controlled environment. This is a good way to get started and for local development. The second one is to compile the dependencies and code outside of the docker container, which is the preferred way for large-scale simulations.","title":"Compiling the code"},{"location":"compiling/#docker-vscode","text":"Within Visual Studio Code, you can use the remote container extension to open, build and run the code directly into a Docker container. To do so, install the Remote Container extension in VSCode, and open the folder where you cloned the murphy code in VSCode. A message will pop up that you can accept, which will download and build the container, and open a terminal in that container. You can go in that terminal, navigate to the main murphy folder, and type ` MAKE_ARCH = make_arch/make.docker_gcc make -j and compilation should start and finish without errors.","title":"Docker + VSCode"},{"location":"compiling/#direct-compilation","text":"To compile the code natively on any machine, you have to first download and install the required dependencies . Once you have those, you can create your own architecture file for make to call, which should be placed in the make_arch/ folder. You can take a look at some of the examples that are there. Specifically, you can specify the following variables: flags CXX_FLAGS : the compilation flags you want to use (default: none ) GIT_COMMIT : the commit associated to the current version of the code (default: taken from the current repo using git describe --always --dirty ) P4est library P4EST_INC : the folder with the p4est headers (default /usr/include ) P4EST_LIB : the folder with the p4est libraries (default /usr/lib ) P4EST_LIBNAME : the name of the p4est library on your system (default: -lsc -lp4est ) HDF5 library HDF5_INC : the folder with the hdf5 headers (default /usr/include ) HDF5_LIB : the folder with the hdf5 libraries (default /usr/lib ) HDF5_LIBNAME : the name of the hdf5 library on your system (default: -lhdf5 ) FFTW library FFTW_INC : the folder with the fftw headers (default /usr/include ) FFTW_LIB : the folder with the fftw libraries (default /usr/lib ) FFTW_LIBNAME : the name of the fftw library on your system (default: -lfftw3_omp -lfftw3 ) Google Test library GTEST_INC : the folder with the google test headers (default /usr/include ) GTEST_LIB : the folder with the google test libraries (default /usr/lib ) GTEST_LIBNAME : the name of the google test library on your system (default: -lgtest )","title":"Direct compilation"},{"location":"compiling/#testing","text":"See the details about automatic testing in this page . To run the tests, you need to recompile all the sources (it's important to delete the old ones if you want to retrieve) make destroy ARCH_FILE=make_arch/make.docker_gcc_test make test -j Then you can lunch the tests ./murphy_test","title":"Testing"},{"location":"compiling/#documentation","text":"To get the doxygen documentation, go to the main folder and enter doxygen doc/Doxyfile You will need dot to get a visual graphs (see HAVE_DOT option in the Doxyfile). On MacOS, you can install it using homebrew: brew install graphviz .","title":"Documentation"},{"location":"compiling/#docker-troubleshoot","text":"the compilation fails and the error indicates that files change sizes during the compilation: you might be the victim of slow disk access between docker and your laptop. To solve that, you need to follow this and set the consistency to delegated .","title":"Docker troubleshoot"},{"location":"compiling/#building-on-clusters","text":"here are the links to the details needed to build murphy on different clusters - Engaging - NIC 5 - Lemaitre 3","title":"Building on clusters"},{"location":"custom-types/","text":"Custom Types To simplify the development of the code, we use a few custom types and macros: here is a summar and how/when to use them: What ? When TYPES rank_t an MPI-rank lda_t a leading dimension numbers (typically 0,1,2) iblock_t a local block index iface_t a face id (0 to 26) level_t a level qdrt_t a p8est_quadrant_t variable real_t a floating point number data_ptr a memory address of the element (0,0,0) mem_ptr the raw memory address const_mem_ptr the constant raw memory address MACROS m_verb displays information in the body of a function or when some diag of the function are not important m_log displays important information at the end of a function, to give info to the user m_min returns the min of two numbers m_max returns the max of two numbers m_sign returns the sign m_pos position of a 3D point in the domain m_pos_relative position of a 3D point in the block m_begin starts every function to have a timer in full verbose mode m_end ends every function to have a timer in full verbose mode MEMORY m_calloc allocate aligned memory m_free free aligned memory m_isaligned check the alignment m_assume_aligned tells the compiler the array is aligned (and check the assertion in debug mode) m_blockmemsize returns the number of elements in one block m_zeroidx shifts the memory to the element (0,0,0)","title":"Custom Types"},{"location":"custom-types/#custom-types","text":"To simplify the development of the code, we use a few custom types and macros: here is a summar and how/when to use them: What ? When TYPES rank_t an MPI-rank lda_t a leading dimension numbers (typically 0,1,2) iblock_t a local block index iface_t a face id (0 to 26) level_t a level qdrt_t a p8est_quadrant_t variable real_t a floating point number data_ptr a memory address of the element (0,0,0) mem_ptr the raw memory address const_mem_ptr the constant raw memory address MACROS m_verb displays information in the body of a function or when some diag of the function are not important m_log displays important information at the end of a function, to give info to the user m_min returns the min of two numbers m_max returns the max of two numbers m_sign returns the sign m_pos position of a 3D point in the domain m_pos_relative position of a 3D point in the block m_begin starts every function to have a timer in full verbose mode m_end ends every function to have a timer in full verbose mode MEMORY m_calloc allocate aligned memory m_free free aligned memory m_isaligned check the alignment m_assume_aligned tells the compiler the array is aligned (and check the assertion in debug mode) m_blockmemsize returns the number of elements in one block m_zeroidx shifts the memory to the element (0,0,0)","title":"Custom Types"},{"location":"dependencies/","text":"Dependencies The compilation uses a standard Makefile , which will call a specified MPI-based, C++-17 compiler. Our dependencies are: Mandatory: make C++-17 compiler : we typically use the GNU C++ compiler. OpenMPI version >=4.1.2 : for parallel communication 1 p4est version XX : for the tree-based operations hdf5 version >=1.10 : for the I/O operations Optional: googletest : for the regression tests flups : a FFT-based Poisson solver for uniform grids. hypre version >=2.23 : a MG-based Poisson solver for uniform grids. MPICH should in principle work, but we haven't exhaustively tested ths. \u21a9","title":"Dependencies"},{"location":"dependencies/#dependencies","text":"The compilation uses a standard Makefile , which will call a specified MPI-based, C++-17 compiler. Our dependencies are: Mandatory: make C++-17 compiler : we typically use the GNU C++ compiler. OpenMPI version >=4.1.2 : for parallel communication 1 p4est version XX : for the tree-based operations hdf5 version >=1.10 : for the I/O operations Optional: googletest : for the regression tests flups : a FFT-based Poisson solver for uniform grids. hypre version >=2.23 : a MG-based Poisson solver for uniform grids. MPICH should in principle work, but we haven't exhaustively tested ths. \u21a9","title":"Dependencies"},{"location":"development-guide/","text":"Development Guide Git management We use the Git Flow approach to maintain a hierarchy and a clean repository. It means that we have the following branches: - master is the default working branch - develop is the development branch, the source branch for every new development - dev-* are the ongoing development branch - fix-* fix a bug or solve an issue To create a new development branch, simply do # go on the develop branch git checkout develop # update your version git pull # create a new branch git checkout -b dev-mynewfeature develop To incorporate your development into the develop branch, use pull requests. The automatic testing must be included and succeed in the code. Additionally, for the commit we use the gitmoji guide to describe your commit purpose. It helps to automatically identify the reason of the commit. Here is a small non-exhaustive list Action Corresponding emoji Solve a regular bug :bug: Sovle a critical bug :ambulance: Add documentation / comments / doxygen :memo: Compilation / makefile :wrench: Docker :whale: Refactor :recycle: Work in Progress :construction: Cleanup the code :wastebasket: Refactor the code :::building_construction::: Create issues to keep track of the development and also to discuss any question you might have. You can use the keywords clos(e/es/ed) , resolv(e/es/ed) and fix(/es/ed) followed by the issue number to close an issue once merged in the default branch git commit -m \":sparkles: closes #9\" Style Guide To ensure a consistent style across murphy, we rely on the Google C++ Style Guide . Additionally, we use two tools to help the formatting: cpplint tool to detect style errors, cpplint --filter = -whitespace/line_length,-runtime/printf myfile clang-format to help the formatting in VSCode. The file .clang-format should be automatically detected and loaded in VSCode, giving you access Immediately to the formatting behavior. You regenerate that file using # brew install clang-format if needed clang-format -style = '{BasedOnStyle: Google, ColumnLimit: 0, IndentWidth: 4, AlignConsecutiveAssignments: true, AlignConsecutiveDeclarations: true, AlignEscapedNewlines: true, AlignOperands: true }' -dump-config > .clang-format There are some exceptions to the style guide as listed below: typedef defined types have a postfix: *_t (e.g. real_t for the real datatype, iface_t when lopping on the faces, ...) the macro definitions acting as functions have a prefix m_ , which stands for MURPHY (e.g. m_verb ) the files are names in lowercase with a .cpp and .hpp extension, with filenames copying the class it contains (e.g. class Grid in grid.cpp ) callback functions used to interface with p4est start with cback_","title":"Development Guide"},{"location":"development-guide/#development-guide","text":"","title":"Development Guide"},{"location":"development-guide/#git-management","text":"We use the Git Flow approach to maintain a hierarchy and a clean repository. It means that we have the following branches: - master is the default working branch - develop is the development branch, the source branch for every new development - dev-* are the ongoing development branch - fix-* fix a bug or solve an issue To create a new development branch, simply do # go on the develop branch git checkout develop # update your version git pull # create a new branch git checkout -b dev-mynewfeature develop To incorporate your development into the develop branch, use pull requests. The automatic testing must be included and succeed in the code. Additionally, for the commit we use the gitmoji guide to describe your commit purpose. It helps to automatically identify the reason of the commit. Here is a small non-exhaustive list Action Corresponding emoji Solve a regular bug :bug: Sovle a critical bug :ambulance: Add documentation / comments / doxygen :memo: Compilation / makefile :wrench: Docker :whale: Refactor :recycle: Work in Progress :construction: Cleanup the code :wastebasket: Refactor the code :::building_construction::: Create issues to keep track of the development and also to discuss any question you might have. You can use the keywords clos(e/es/ed) , resolv(e/es/ed) and fix(/es/ed) followed by the issue number to close an issue once merged in the default branch git commit -m \":sparkles: closes #9\"","title":"Git management"},{"location":"development-guide/#style-guide","text":"To ensure a consistent style across murphy, we rely on the Google C++ Style Guide . Additionally, we use two tools to help the formatting: cpplint tool to detect style errors, cpplint --filter = -whitespace/line_length,-runtime/printf myfile clang-format to help the formatting in VSCode. The file .clang-format should be automatically detected and loaded in VSCode, giving you access Immediately to the formatting behavior. You regenerate that file using # brew install clang-format if needed clang-format -style = '{BasedOnStyle: Google, ColumnLimit: 0, IndentWidth: 4, AlignConsecutiveAssignments: true, AlignConsecutiveDeclarations: true, AlignEscapedNewlines: true, AlignOperands: true }' -dump-config > .clang-format There are some exceptions to the style guide as listed below: typedef defined types have a postfix: *_t (e.g. real_t for the real datatype, iface_t when lopping on the faces, ...) the macro definitions acting as functions have a prefix m_ , which stands for MURPHY (e.g. m_verb ) the files are names in lowercase with a .cpp and .hpp extension, with filenames copying the class it contains (e.g. class Grid in grid.cpp ) callback functions used to interface with p4est start with cback_","title":"Style Guide"},{"location":"gettingstarted/","text":"Getting started Here we go over some of the main, high-level operations required to perform a simple operation in MURPHY. The full code is here.. 1. Grid initialization int L [ 3 ] = { 2 , 1 , 3 }; bool periodic [ 3 ] = { false , false , false }; Grid * grid = new Grid ( 1 , periodic , L , M_GRIDBLOCK , MPI_COMM_WORLD , nullptr ); We create a distributed (MPI) forest of octrees, organized in a rectangular shape. The number of tree in each direction is given by L[0] x L[1] x L[2] (each octree is a unit cube of physical size 1 x 1 x 1 ); The tree management operations are performed by the library p4est . Additionally to the forest size, the periodicity of its boundary is given to p4est and cannot change during the simulation. Other boundary conditions are given afterwards. Each leaf of the tree contains a GridBlock , a continuous memory space of fixed size. The block size in a direction is given by the number of unknowns in its core, M_N , and the number of ghost points, M_GS . The memory stride in each direction is then given by M_STRIDE = 2 * M_GS + M_N . 2. Field creation Field * vort = new Field ( \"vorticity\" , 3 ); This command defines a field by a unique name , here vorticity , and a dimensionality lda , here 3 . The field does not contain its own memory and is almost an empty shell. However, its name and lda are used each time one wants to perform an operation on it. 3. Grid-Field association grid -> AddField ( vort ); The association of a field to a grid will perform the real memory allocation. A field can be associated to multiple grids and each grid contains a map of all the fields it is associated with. When a field is added to a grid, this will trigger a loop on all the grid blocks to initialize the memory. The memory is allocated continuously, each component separated by M_STRIDE * M_STRIDE * M_STRIDE . This means that, in memory, we have [Field(ida=0) Field(ida=1) Field(ida=2)] in one continuous array. 4. Boundary conditions vort -> bctype ( M_BC_ODD ); The imposition of a boundary condition is done on the field. Each dimension, ida , will get 6 boundary conditions, one for each of the faces. These are ordered as follows: x- , x+ , y- , y+ , z- , z+ . The boundary conditions will be used any time ghost points are needed for the field. If a simulation direction is periodic, which is set globally on the grid and not on individual fields , the boundary conditions on the field in that direction will be discarded automatically. 5. Fill the field with values 7. Grid adaptation 8. DoOp functions MURPHY implements three high-level functions to loops on the GridBlocks : DoOpMesh : loops on the blocks using the p4est_mesh , i.e. requires the ghosts to be up to date with the grid (not the ghost values!) DoOpMeshLevel : same but only on a considered level DoOpTree : loops on the blocks using the trees, i.e. do not require the ghosts to be up to date with the grid. The functions operates using a similar interface: an object that owns the function you want to call on the blocks the function of the object that will be called, as a lambda function the grid any other arguments that the user wants forward to the function. For example, here is a list of different values it's the responsibility of every operation that involves a DoOp , to update the ghost status of a field once the job is done. 9. Ghost update grid -> GhostPull ( vort ); The ghost points computation is done using the wavelets to reconstruct missing information in the case of a level mismatch. The ghost reconstruction is done dimension by dimension, allowing to overlap wavelet reconstruction with the communication for the next dimension. Each field owns a boolean to indicate if the ghost points are up-to-date. Hence, calling the GhostPull with an already up-to-date field will return immediately. To ensure consistency of this boolean, please make sure the operator classes set this value accurately. 10. Dump","title":"Getting started"},{"location":"gettingstarted/#getting-started","text":"Here we go over some of the main, high-level operations required to perform a simple operation in MURPHY. The full code is here..","title":"Getting started"},{"location":"gettingstarted/#1-grid-initialization","text":"int L [ 3 ] = { 2 , 1 , 3 }; bool periodic [ 3 ] = { false , false , false }; Grid * grid = new Grid ( 1 , periodic , L , M_GRIDBLOCK , MPI_COMM_WORLD , nullptr ); We create a distributed (MPI) forest of octrees, organized in a rectangular shape. The number of tree in each direction is given by L[0] x L[1] x L[2] (each octree is a unit cube of physical size 1 x 1 x 1 ); The tree management operations are performed by the library p4est . Additionally to the forest size, the periodicity of its boundary is given to p4est and cannot change during the simulation. Other boundary conditions are given afterwards. Each leaf of the tree contains a GridBlock , a continuous memory space of fixed size. The block size in a direction is given by the number of unknowns in its core, M_N , and the number of ghost points, M_GS . The memory stride in each direction is then given by M_STRIDE = 2 * M_GS + M_N .","title":"1. Grid initialization"},{"location":"gettingstarted/#2-field-creation","text":"Field * vort = new Field ( \"vorticity\" , 3 ); This command defines a field by a unique name , here vorticity , and a dimensionality lda , here 3 . The field does not contain its own memory and is almost an empty shell. However, its name and lda are used each time one wants to perform an operation on it.","title":"2. Field creation"},{"location":"gettingstarted/#3-grid-field-association","text":"grid -> AddField ( vort ); The association of a field to a grid will perform the real memory allocation. A field can be associated to multiple grids and each grid contains a map of all the fields it is associated with. When a field is added to a grid, this will trigger a loop on all the grid blocks to initialize the memory. The memory is allocated continuously, each component separated by M_STRIDE * M_STRIDE * M_STRIDE . This means that, in memory, we have [Field(ida=0) Field(ida=1) Field(ida=2)] in one continuous array.","title":"3. Grid-Field association"},{"location":"gettingstarted/#4-boundary-conditions","text":"vort -> bctype ( M_BC_ODD ); The imposition of a boundary condition is done on the field. Each dimension, ida , will get 6 boundary conditions, one for each of the faces. These are ordered as follows: x- , x+ , y- , y+ , z- , z+ . The boundary conditions will be used any time ghost points are needed for the field. If a simulation direction is periodic, which is set globally on the grid and not on individual fields , the boundary conditions on the field in that direction will be discarded automatically.","title":"4. Boundary conditions"},{"location":"gettingstarted/#5-fill-the-field-with-values","text":"","title":"5. Fill the field with values"},{"location":"gettingstarted/#7-grid-adaptation","text":"","title":"7. Grid adaptation"},{"location":"gettingstarted/#8-doop-functions","text":"MURPHY implements three high-level functions to loops on the GridBlocks : DoOpMesh : loops on the blocks using the p4est_mesh , i.e. requires the ghosts to be up to date with the grid (not the ghost values!) DoOpMeshLevel : same but only on a considered level DoOpTree : loops on the blocks using the trees, i.e. do not require the ghosts to be up to date with the grid. The functions operates using a similar interface: an object that owns the function you want to call on the blocks the function of the object that will be called, as a lambda function the grid any other arguments that the user wants forward to the function. For example, here is a list of different values it's the responsibility of every operation that involves a DoOp , to update the ghost status of a field once the job is done.","title":"8. DoOp functions"},{"location":"gettingstarted/#9-ghost-update","text":"grid -> GhostPull ( vort ); The ghost points computation is done using the wavelets to reconstruct missing information in the case of a level mismatch. The ghost reconstruction is done dimension by dimension, allowing to overlap wavelet reconstruction with the communication for the next dimension. Each field owns a boolean to indicate if the ghost points are up-to-date. Hence, calling the GhostPull with an already up-to-date field will return immediately. To ensure consistency of this boolean, please make sure the operator classes set this value accurately.","title":"9. Ghost update"},{"location":"gettingstarted/#10-dump","text":"","title":"10. Dump"},{"location":"multiresolution-grids/","text":"Multiresolution Adaptive Grids The scientific paper can be found here while the Github code is here . Approach MURPHY uses block-structured grids organized in octree data structures. Each block contains a fixed number of \\(N_b\\) grid points in each direction at a uniform resolution. Grid adaptation is performed by splitting an existing grid block into eight finer grid blocks, or combining eight existing grid blocks into one coarser grid block. To decide when to refine or compress, MURPHY regularly performs a wavelet transform of key indicator fields. With this transform MURPHY can quantify the smoothness of the field as compared to a chosen polynomial basis, and this smoothness is used to decide whether each blocks can compress, should refine, or stay at the same level of resolution. We further use the wavelet basis to consistently create or discard information when adapting the grid, as well as create ghost points for blocks neighboring resolution jumps. Mathematical basis","title":"Multiresolution grid"},{"location":"multiresolution-grids/#multiresolution-adaptive-grids","text":"The scientific paper can be found here while the Github code is here .","title":"Multiresolution Adaptive Grids"},{"location":"multiresolution-grids/#approach","text":"MURPHY uses block-structured grids organized in octree data structures. Each block contains a fixed number of \\(N_b\\) grid points in each direction at a uniform resolution. Grid adaptation is performed by splitting an existing grid block into eight finer grid blocks, or combining eight existing grid blocks into one coarser grid block. To decide when to refine or compress, MURPHY regularly performs a wavelet transform of key indicator fields. With this transform MURPHY can quantify the smoothness of the field as compared to a chosen polynomial basis, and this smoothness is used to decide whether each blocks can compress, should refine, or stay at the same level of resolution. We further use the wavelet basis to consistently create or discard information when adapting the grid, as well as create ghost points for blocks neighboring resolution jumps.","title":"Approach"},{"location":"multiresolution-grids/#mathematical-basis","text":"","title":"Mathematical basis"}]}